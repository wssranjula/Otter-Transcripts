# ğŸš€ Streamlit RAG Chatbot - User Guide

A modern, user-friendly interface for your Meeting Knowledge Chatbot with full visibility into the RAG (Retrieval-Augmented Generation) process.

## âœ¨ Features

### 1. **Interactive Chat Interface**
- Clean, modern UI built with Streamlit
- Real-time question answering
- Chat history tracking
- Sample questions for quick start

### 2. **Retrieved Context Visibility**
- See exactly which chunks the AI is using to answer your questions
- View metadata for each chunk:
  - Meeting title and date
  - Speakers involved
  - Chunk type (decision, action, assessment, etc.)
  - Importance score
  - Entities mentioned

### 3. **Customizable Settings**
- Adjust number of chunks retrieved (3-15)
- Toggle context visibility on/off
- Session statistics tracking

### 4. **User-Friendly Design**
- Two-column layout: Chat on left, Context on right
- Expandable chunk cards for detailed viewing
- Color-coded importance and types
- Responsive design

---

## ğŸš€ How to Run

### Step 1: Install Streamlit (if not already installed)

```bash
pip install streamlit
```

### Step 2: Run the Chatbot

```bash
streamlit run streamlit_chatbot.py
```

The app will automatically open in your browser at `http://localhost:8501`

---

## ğŸ’¡ How to Use

### Asking Questions

1. **Type your question** in the text area on the left
2. **Click "ğŸš€ Ask"** button
3. **View the answer** generated by the AI
4. **Explore retrieved chunks** on the right to see what context was used

### Using Sample Questions

Click any of the sample questions in the sidebar to quickly test the system:
- "What were the key decisions in the last meeting?"
- "What actions were assigned to Tom?"
- "What is our Germany strategy?"
- etc.

### Adjusting Settings

**In the sidebar:**
- **Number of chunks to retrieve:**
  - Lower (3-5): Faster, more focused answers
  - Higher (10-15): More comprehensive, slower answers

- **Show retrieved chunks:** Toggle to show/hide the context used

### Understanding the Context Display

Each chunk card shows:
- **Header:** Meeting title and date
- **Metadata:**
  - Type: decision, action_assignment, assessment, discussion, question
  - Importance: 0.0 to 1.0 (higher = more important)
  - Speakers: Number of unique speakers
- **Speakers List:** Names of people who spoke in this chunk
- **Content:** The actual text from the meeting
- **Entities:** Key people, organizations, countries, topics mentioned

---

## ğŸ¯ Example Workflow

### Example 1: Understanding a Decision

**Question:** "Why did we deprioritize Germany?"

**What happens:**
1. System retrieves 8 most relevant chunks about Germany
2. Shows you chunks containing:
   - Tom Pravda's assessment of Germany
   - Decision to deprioritize
   - Rationale about anti-SRM NGO risks
3. AI synthesizes answer from these chunks
4. You can read the original context to verify

### Example 2: Finding Action Items

**Question:** "What tasks are assigned to Ben Margetts?"

**What happens:**
1. System searches for action_assignment chunks
2. Filters for Ben Margetts as owner
3. Shows action items with meeting context
4. AI lists all tasks with deadlines and context

### Example 3: Temporal Queries

**Question:** "What was discussed in the June 11 meeting?"

**What happens:**
1. System detects date pattern "June 11"
2. Retrieves chunks specifically from 2025-06-11
3. Sorts by importance (decisions/actions first)
4. AI provides comprehensive meeting summary

---

## âš™ï¸ Configuration

### Update Database Credentials

Edit `streamlit_chatbot.py` lines 40-44:

```python
NEO4J_URI = "bolt://your-database.databases.neo4j.io:7687"
NEO4J_USER = "neo4j"
NEO4J_PASSWORD = "your-password"
MISTRAL_API_KEY = "your-mistral-api-key"
MODEL = "mistral-small-latest"  # or mistral-large-latest
```

### Adjust Default Settings

In the code (lines 97-102):

```python
context_limit = st.slider(
    "Number of chunks to retrieve",
    min_value=3,
    max_value=15,
    value=8,  # Change default here
    help="More chunks = more context but slower responses"
)
```

---

## ğŸ¨ UI Layout

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           Meeting Knowledge Chatbot                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                  â”‚                                  â”‚
â”‚  ğŸ’¬ Ask Question â”‚  ğŸ“š Retrieved Context Chunks     â”‚
â”‚                  â”‚                                  â”‚
â”‚  [Text Input]    â”‚  [Chunk 1: Meeting - Date]       â”‚
â”‚  [ğŸš€ Ask Button] â”‚   â€¢ Type: decision               â”‚
â”‚                  â”‚   â€¢ Importance: 0.92             â”‚
â”‚  âœ¨ Latest Answerâ”‚   â€¢ Content: ...                 â”‚
â”‚                  â”‚                                  â”‚
â”‚  [Answer Display]â”‚  [Chunk 2: Meeting - Date]       â”‚
â”‚                  â”‚   â€¢ Type: assessment             â”‚
â”‚                  â”‚   â€¢ Importance: 0.85             â”‚
â”‚                  â”‚   â€¢ Content: ...                 â”‚
â”‚                  â”‚                                  â”‚
â”‚                  â”‚  [Chunk 3: ...]                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ğŸ“œ Chat History (Last 5 Q&A pairs)                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Sidebar:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ âš™ï¸ Settings   â”‚
â”‚ â€¢ Chunks: [8] â”‚
â”‚ â€¢ Show â˜‘ï¸     â”‚
â”‚              â”‚
â”‚ ğŸ“Š Stats      â”‚
â”‚ Questions: 3  â”‚
â”‚              â”‚
â”‚ ğŸ’¡ Samples    â”‚
â”‚ [Button]      â”‚
â”‚ [Button]      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ” Understanding Chunk Types

| Type | Description | Example |
|------|-------------|---------|
| **decision** | Decisions made | "We decided to deprioritize Germany" |
| **action_assignment** | Tasks assigned to people | "Tom will lead HAC strategy" |
| **assessment** | Analysis or evaluation | "Germany is too risky due to NGOs" |
| **discussion** | General conversation | "We need to think about funding" |
| **question** | Questions raised | "What about the UK timeline?" |

---

## ğŸ› Troubleshooting

### App won't start
- **Check:** Is streamlit installed? `pip install streamlit`
- **Check:** Are you in the correct directory?

### Connection errors
- **Check:** Neo4j database credentials in the code
- **Check:** Internet connection for Neo4j Aura
- **Check:** Mistral API key is valid

### No chunks displayed
- **Check:** "Show retrieved chunks" is enabled in sidebar
- **Check:** Database has data loaded
- **Try:** Asking a different question

### Slow responses
- **Reduce** number of chunks to retrieve (3-5 instead of 10-15)
- **Check:** Mistral API rate limits
- **Try:** Using `mistral-small-latest` instead of `mistral-large-latest`

---

## ğŸ¯ Tips for Best Results

### 1. **Be Specific**
- âŒ "Tell me about meetings"
- âœ… "What decisions were made about the High Ambition Coalition in June?"

### 2. **Use Entity Names**
- Mention specific people: "Tom Pravda", "Ben Margetts"
- Mention organizations: "Heinrich BÃ¶ll Foundation"
- Mention countries: "Germany", "Kenya"

### 3. **Use Temporal Queries**
- "last meeting", "recent decisions", "latest update"
- Specific dates: "June 11", "May 28"

### 4. **Check the Context**
- Always review the retrieved chunks
- Verify the AI used relevant context
- If context seems off, rephrase your question

### 5. **Adjust Chunk Limit**
- Simple questions: 3-5 chunks
- Complex questions: 8-12 chunks
- Comprehensive summaries: 12-15 chunks

---

## ğŸ“Š Understanding Performance

### Retrieval Time
- **3-5 chunks:** ~0.5-1 second
- **8-10 chunks:** ~1-2 seconds
- **12-15 chunks:** ~2-3 seconds

### Answer Generation Time
- **mistral-small-latest:** ~2-4 seconds
- **mistral-large-latest:** ~4-8 seconds

### Total Response Time
- **Typical:** 3-5 seconds
- **With many chunks:** 5-10 seconds

---

## ğŸš€ Next Steps

Want to improve the system further? See:
- `IMPROVE_RETRIEVAL_QUALITY.md` - Strategies to improve context retrieval
- `CHATBOT_README.md` - Command-line chatbot usage
- `CONNECTION_TIMEOUT_FIX.md` - Understanding the batch processing fix

---

## ğŸ“ Notes

- The chatbot uses RAG (Retrieval-Augmented Generation)
- All answers are grounded in actual meeting transcripts
- The AI cannot make up information - it only uses retrieved context
- Chat history is stored in session (cleared on page refresh)

---

**Enjoy your enhanced chatbot experience!** ğŸ‰

For questions or issues, check the documentation files in this directory.
